Vocab sizes with threshold 2, sentence lengths 16,19
- Cornell ~25k
- Our data ~33k

Corpus sizes, no length limit:
- Cornell ~ 190k
- Our data ~ 360k

Corpus sizes, sentence lenghts 16,19:
- Cornell ~ 105k
- Our data ~ 170k 

Problems:
- <person> tag in our data gets split up into three tokens!
- Regular softmax version fails terribly on small datasets. Maybe due to the dirty fix of the hidden layer size :(

Include option to reuse the processed version of the corpus, outside of the models folder 

Add weighting of padding to avoid training on padding loss?


- Adapt Cornell dataset to test mode
- Make it possible to use both datasets at once
- Implement vector extrama metric.
- Use perplexity metric for validation?

- Full run on the baseline (our data)
- Measure/quantify the limitations of the baseline model


###################################
Stage2:
- Re-include attention (one line) - Should we do this now? Maybe that would help improve the quality of the model
- extend model with other idea(s)

-extend training to triples
