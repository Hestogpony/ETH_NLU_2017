Vocab sizes with threshold 2, sentence lengths 16,19
- Cornell ~25k
- Our data ~33k

Corpus sizes, no length limit:
- Cornell ~ 190k
- Our data ~ 360k

Corpus sizes, sentence lenghts 16,19:
- Cornell ~ 105k
- Our data ~ 170k 



Problems:
- LOW PRIO: <person> tag in our data gets split up into three tokens!
- LOW PRIO: Regular softmax version fails terribly on small datasets. Maybe due to the dirty fix of the hidden layer size :(



TODO:

- Add another parameter for the embedding size --> Done
	--> Maybe add another fully connected layer for "output embeddings"

- Build vocabulary based on the sentences that actually fit in the buckets.
	Pro: Smaller input and output layer, should be a little bit faster
	Con: Each max bucket size change requires reloading the entire dataset

- Save model on exit


- Make it possible to use both datasets at once
- Implement vector extrema metric.

- Measure/quantify the limitations of the baseline model
	--> inituitive: Variety of the answers is quite small. All the answers are pretty generic and short

Param tuning:
- Increase threshold for entry in vocabulary
- Lower drop out rate --> No, I think we have an overfitting problem now
- Only one short bucket, fuck all the other data.
- Increase Hidden State size of GRU
- Gradient norm

###################################
Stage2:
- extend model with other idea(s)
	- reinforcement approach
	- 5. Different objective function
	- Maybe: Handling of unks (7), consistent model for input (4) 
	- Beam search. WTF is this?
	- Google for more stuff!

- extend training to triples (both our dataset and cornell)
