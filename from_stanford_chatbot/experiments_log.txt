June 4:
- Big model works ok after some iterations, but overfits afterwards
- A model with only one bucket (8,10) overfits even more

June 5:
- Smaller vocab size (threshold 10), dropout 0.5 --> not really better, starts overfitting pretty soon
- 4 layer GRU, dropout 0.6, embeddings 200, adam optimizer, clipped gradients to 1.
		Continuation: Gradient norm was maybe too small. Changed it to 2. Now it takes 25% longer to train. Batch size 128


