What exactly do we need?

    READER:
        Data triples: UTTERANCE 1  |  UTTERANCE 2  |  UTTERANCE 3
        separated by tabs, one per line

        At first, we'll transform this into tuples
            UTTERANCE 1 | UTTERANCE 2
            UTTERANCE 2 | UTTERANCE 3

        But after that it could be possible to use the triples instead

        ----

        As usual, generate a dictionary, specifying the max vocab size

        BOS: signals the transition from encode to decode; place there manually
        EOS: stop when this is generated by the decoder
        UNK: appears in place of the oov words
        PAD: should be given

        In the end, if you have a pair:

        A B C | X Y Z W

        you feed A B C into the encoder, and then <BOS> X Y Z W into the decoder

        The decoder targets are X Y Z W <EOS>

        Note that the input and output vocabs can be different, but perhaps we
        should keep them the same for now.

        In the tutorial, they also have weights that they set to zero for
        padding. That way the scores for padding output are ignored.

    Bucketing:
        This is explained in the tensorflow seq2seq tutorial, but the idea is
        that instead of having one model that we need to unroll a lot for the
        longest sentences, we have multiple unroll possibilities for input and
        output.

        Then when padding we pad to the next bucket up

    Model:
        The tutorial suggests at least a couple recurrent layers with size 1024.